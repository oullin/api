# DB: Define the source of the secrets on the host machine.
secrets:
    pg_username:
        file: ${DB_SECRET_USERNAME:-./database/infra/secrets/pg_username}
    pg_password:
        file: ${DB_SECRET_PASSWORD:-./database/infra/secrets/pg_password}
    pg_dbname:
        file: ${DB_SECRET_DBNAME:-./database/infra/secrets/pg_dbname}

volumes:
    caddy_data:
    caddy_config:
    go_mod_cache:
        driver: local
    prometheus_data_prod:
        driver: local
    prometheus_data_local:
        driver: local
    grafana_data_prod:
        driver: local
    grafana_data_local:
        driver: local
    loki_data_prod:
        driver: local
    loki_data_local:
        driver: local
    tempo_data_prod:
        driver: local
    tempo_data_local:
        driver: local

    # --- DB: Define a named volume at the top level.
    #     Docker will manage its lifecycle.
    oullin_db_data:
        driver: local

networks:
    caddy_net:
        name: caddy_net
        external: true
    oullin_net:
        name: oullin_net
        driver: bridge

services:
        caddy_prod:
            image: api-caddy_prod
            build:
                context: ./infra/caddy
                dockerfile: Dockerfile
                args:
                    - CADDY_VERSION=2.10.2

            profiles: ["prod"]
            container_name: oullin_proxy_prod
            restart: unless-stopped
            depends_on:
                - api

            # --- The 443:443/udp is required for HTTP/3
            #     NOTES:
            #           - Admin API (2019) listens on all interfaces but is NOT published to host
            #           - Prometheus scrapes metrics from dedicated endpoint (9180) via Docker internal DNS
            ports:
                - "80:80"
                - "443:443"
                - "443:443/udp" # Required for HTTP/3
                # NOTE: Admin API (2019) is NOT published to host (internal Docker network only)
                # Prometheus scrapes Caddy metrics from :9180 via Docker internal DNS

            # --- Dedicated /metrics endpoint for Prometheus (internal network only)
            expose:
                - "9180"
            volumes:
                - caddy_data:/data
                - caddy_config:/config
                - ./infra/caddy/Caddyfile.prod:/etc/caddy/Caddyfile
                - ${CADDY_LOGS_PATH}:/var/log/caddy
                - ./infra/caddy/mtls:/etc/caddy/mtls:ro
            networks:
                caddy_net:
                  aliases:
                    - proxy

        caddy_local:
            build:
                context: ./infra/caddy
                dockerfile: Dockerfile
                args:
                    - CADDY_VERSION=2.10.2

            profiles: ["local"]
            container_name: oullin_local_proxy
            restart: unless-stopped
            depends_on:
                - api
            ports:
                - "18080:80"
                - "8443:443"
                - "127.0.0.1:2019:2019" # Admin API - localhost only for debugging

            # --- Dedicated /metrics endpoint for Prometheus (internal network only)
            expose:
                - "9180"

            volumes:
                - caddy_data:/data
                - caddy_config:/config
                - ./infra/caddy/mtls:/etc/caddy/mtls:ro
                - ./infra/caddy/Caddyfile.local:/etc/caddy/Caddyfile
            networks:
                - caddy_net

        prometheus:
            image: prom/prometheus:v3.0.1
            profiles: ["prod"]
            container_name: oullin_prometheus
            restart: unless-stopped
            command:
                - '--config.file=/etc/prometheus/prometheus.yml'
                - '--storage.tsdb.path=/prometheus'
                - '--storage.tsdb.retention.time=30d'
                - '--web.console.libraries=/usr/share/prometheus/console_libraries'
                - '--web.console.templates=/usr/share/prometheus/consoles'
            ports:
                - "127.0.0.1:9090:9090"
            volumes:
                - ./infra/metrics/prometheus/provisioning/prometheus.yml:/etc/prometheus/prometheus.yml:ro
                - prometheus_data_prod:/prometheus
            networks:
                - caddy_net
                - oullin_net
            depends_on:
                caddy_prod:
                    condition: service_started
                postgres_exporter:
                    condition: service_healthy
            healthcheck:
                test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
                interval: 10s
                timeout: 5s
                retries: 5
                start_period: 10s
            deploy:
                resources:
                    limits:
                        cpus: '1.0'
                        memory: 1G
                    reservations:
                        cpus: '0.25'
                        memory: 256M

        prometheus_local:
            image: prom/prometheus:v3.0.1
            profiles: ["local"]
            container_name: oullin_prometheus_local
            restart: unless-stopped
            command:
                - '--config.file=/etc/prometheus/prometheus.yml'
                - '--storage.tsdb.path=/prometheus'
                - '--storage.tsdb.retention.time=7d'
                - '--web.console.libraries=/usr/share/prometheus/console_libraries'
                - '--web.console.templates=/usr/share/prometheus/consoles'
            ports:
                - "9090:9090"
            volumes:
                - ./infra/metrics/prometheus/provisioning/prometheus.local.yml:/etc/prometheus/prometheus.yml:ro
                - prometheus_data_local:/prometheus
            networks:
                - caddy_net
                - oullin_net
            depends_on:
                caddy_local:
                    condition: service_started
                postgres_exporter_local:
                    condition: service_healthy
            healthcheck:
                test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
                interval: 10s
                timeout: 5s
                retries: 5
                start_period: 10s
            deploy:
                resources:
                    limits:
                        cpus: '1.0'
                        memory: 1G
                    reservations:
                        cpus: '0.25'
                        memory: 256M

        postgres_exporter:
            image: prometheuscommunity/postgres-exporter:v0.15.0
            profiles: ["prod"]
            container_name: oullin_postgres_exporter
            restart: unless-stopped
            entrypoint: ["/postgres-exporter-entrypoint.sh"]
            volumes:
                - ./infra/metrics/prometheus/scripts/postgres-exporter-entrypoint.sh:/postgres-exporter-entrypoint.sh:ro
            secrets:
                - pg_username
                - pg_password
                - pg_dbname
            networks:
                - oullin_net
                - caddy_net
            depends_on:
                api-db:
                    condition: service_healthy
            expose:
                - "9187"
            healthcheck:
                test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9187/"]
                interval: 10s
                timeout: 5s
                retries: 5
                start_period: 10s
            deploy:
                resources:
                    limits:
                        cpus: '0.25'
                        memory: 128M
                    reservations:
                        cpus: '0.05'
                        memory: 32M

        postgres_exporter_local:
            image: prometheuscommunity/postgres-exporter:v0.15.0
            profiles: ["local"]
            container_name: oullin_postgres_exporter_local
            restart: unless-stopped
            entrypoint: ["/postgres-exporter-entrypoint.sh"]
            volumes:
                - ./infra/metrics/prometheus/scripts/postgres-exporter-entrypoint.sh:/postgres-exporter-entrypoint.sh:ro
            secrets:
                - pg_username
                - pg_password
                - pg_dbname
            networks:
                - oullin_net
                - caddy_net
            depends_on:
                api-db:
                    condition: service_healthy
            expose:
                - "9187"
            healthcheck:
                test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9187/"]
                interval: 10s
                timeout: 5s
                retries: 5
                start_period: 10s
            deploy:
                resources:
                    limits:
                        cpus: '0.25'
                        memory: 128M
                    reservations:
                        cpus: '0.05'
                        memory: 32M

        grafana:
            image: grafana/grafana:11.4.0
            profiles: ["prod"]
            container_name: oullin_grafana
            restart: unless-stopped
            ports:
                - "127.0.0.1:3000:3000"
            environment:
                - GF_SERVER_ROOT_URL=http://localhost:3000
                - GF_SECURITY_ADMIN_USER=admin
                - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
                - GF_USERS_ALLOW_SIGN_UP=false
                - GF_AUTH_ANONYMOUS_ENABLED=false
                - GF_INSTALL_PLUGINS=
                - GF_DATASOURCE_PROMETHEUS_URL=http://oullin_prometheus:9090
                - GF_DATASOURCE_LOKI_URL=http://oullin_loki:3100
                - GF_DATASOURCE_TEMPO_URL=http://oullin_tempo:3200
            volumes:
                - grafana_data_prod:/var/lib/grafana
                - ./infra/metrics/grafana/provisioning:/etc/grafana/provisioning:ro
                - ./infra/metrics/grafana/dashboards:/var/lib/grafana/dashboards:ro
            networks:
                - caddy_net
            depends_on:
                prometheus:
                    condition: service_healthy
                loki:
                    condition: service_healthy
                tempo:
                    condition: service_healthy
            healthcheck:
                test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/"]
                interval: 10s
                timeout: 5s
                retries: 5
                start_period: 30s
            deploy:
                resources:
                    limits:
                        cpus: '0.5'
                        memory: 512M
                    reservations:
                        cpus: '0.1'
                        memory: 128M

        grafana_local:
            image: grafana/grafana:11.4.0
            profiles: ["local"]
            container_name: oullin_grafana_local
            restart: unless-stopped
            ports:
                - "3000:3000"
            environment:
                - GF_SERVER_ROOT_URL=http://localhost:3000
                - GF_SECURITY_ADMIN_USER=admin
                - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
                - GF_USERS_ALLOW_SIGN_UP=false
                - GF_AUTH_ANONYMOUS_ENABLED=false
                - GF_INSTALL_PLUGINS=
                - GF_DATASOURCE_PROMETHEUS_URL=http://oullin_prometheus_local:9090
                - GF_DATASOURCE_LOKI_URL=http://oullin_loki_local:3100
                - GF_DATASOURCE_TEMPO_URL=http://oullin_tempo_local:3200
            volumes:
                - grafana_data_local:/var/lib/grafana
                - ./infra/metrics/grafana/provisioning:/etc/grafana/provisioning:ro
                - ./infra/metrics/grafana/dashboards:/var/lib/grafana/dashboards:ro
            networks:
                - caddy_net
            depends_on:
                prometheus_local:
                    condition: service_healthy
                loki_local:
                    condition: service_healthy
                tempo_local:
                    condition: service_healthy
            healthcheck:
                test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/"]
                interval: 10s
                timeout: 5s
                retries: 5
                start_period: 30s
            deploy:
                resources:
                    limits:
                        cpus: '0.5'
                        memory: 512M
                    reservations:
                        cpus: '0.1'
                        memory: 128M

        loki:
            image: grafana/loki:3.3.2
            profiles: ["prod"]
            container_name: oullin_loki
            restart: unless-stopped
            ports:
                - "127.0.0.1:3100:3100"
            command: -config.file=/etc/loki/local-config.yaml
            volumes:
                - loki_data_prod:/loki
            networks:
                - caddy_net
            healthcheck:
                test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3100/ready"]
                interval: 10s
                timeout: 5s
                retries: 5
                start_period: 10s
            deploy:
                resources:
                    limits:
                        cpus: '0.5'
                        memory: 512M
                    reservations:
                        cpus: '0.1'
                        memory: 128M

        loki_local:
            image: grafana/loki:3.3.2
            profiles: ["local"]
            container_name: oullin_loki_local
            restart: unless-stopped
            ports:
                - "3100:3100"
            command: -config.file=/etc/loki/local-config.yaml
            volumes:
                - loki_data_local:/loki
            networks:
                - caddy_net
            healthcheck:
                test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3100/ready"]
                interval: 10s
                timeout: 5s
                retries: 5
                start_period: 10s
            deploy:
                resources:
                    limits:
                        cpus: '0.5'
                        memory: 512M
                    reservations:
                        cpus: '0.1'
                        memory: 128M

        tempo:
            image: grafana/tempo:2.7.2
            profiles: ["prod"]
            container_name: oullin_tempo
            restart: unless-stopped
            ports:
                - "127.0.0.1:3200:3200"   # tempo
                - "127.0.0.1:4317:4317"   # otlp grpc
                - "127.0.0.1:4318:4318"   # otlp http
            command: ["-config.file=/etc/tempo.yaml"]
            volumes:
                - tempo_data_prod:/tmp/tempo
                - ./infra/metrics/tempo/tempo-config.yaml:/etc/tempo.yaml:ro
            networks:
                - caddy_net
                - oullin_net
            healthcheck:
                test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3200/ready"]
                interval: 10s
                timeout: 5s
                retries: 5
                start_period: 10s
            deploy:
                resources:
                    limits:
                        cpus: '0.5'
                        memory: 512M
                    reservations:
                        cpus: '0.1'
                        memory: 128M

        tempo_local:
            image: grafana/tempo:2.7.2
            profiles: ["local"]
            container_name: oullin_tempo_local
            restart: unless-stopped
            ports:
                - "3200:3200"   # tempo
                - "4317:4317"   # otlp grpc
                - "4318:4318"   # otlp http
            command: ["-config.file=/etc/tempo.yaml"]
            volumes:
                - tempo_data_local:/tmp/tempo
                - ./infra/metrics/tempo/tempo-config.yaml:/etc/tempo.yaml:ro
            networks:
                - caddy_net
                - oullin_net
            healthcheck:
                test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3200/ready"]
                interval: 10s
                timeout: 5s
                retries: 5
                start_period: 10s
            deploy:
                resources:
                    limits:
                        cpus: '0.5'
                        memory: 512M
                    reservations:
                        cpus: '0.1'
                        memory: 128M

        # A dedicated service for running one-off Go commands
        api-runner:
          container_name: runner
          restart: no
          env_file:
            - ./.env
          build:
            context: .
            dockerfile: ./infra/docker/dockerfile-api
            target: builder
          volumes:
            - .:/app
            - go_mod_cache:/go/pkg/mod
            - "${ENV_SPA_DIR}:${ENV_SPA_DIR}"
            - "${ENV_SPA_IMAGES_DIR}:${ENV_SPA_IMAGES_DIR}"
          working_dir: /app
          environment:
              CGO_ENABLED: 1
              ENV_DB_HOST: api-db
              ENV_SPA_DIR: ${ENV_SPA_DIR}
              ENV_DB_PORT: ${ENV_DB_PORT:-5432}
              ENV_PING_USERNAME: ${ENV_PING_USERNAME}
              ENV_PING_PASSWORD: ${ENV_PING_PASSWORD}
              ENV_SPA_IMAGES_DIR: ${ENV_SPA_IMAGES_DIR}
          networks:
            - oullin_net
          secrets:
            - pg_username
            - pg_password
            - pg_dbname
          depends_on:
            api-db:
              condition: service_healthy

        api:
            user: root
            security_opt:
              - apparmor:unconfined
            env_file:
                - .env
            volumes:
              - ./.env:/app/.env:ro
            environment:
                CGO_ENABLED: 1
                # --- This ensures the Go web server listens for connections from other
                #     containers (like Caddy), not just from within itself.
                ENV_DB_HOST: api-db
                ENV_HTTP_HOST: 0.0.0.0
                ENV_TRACING_ENABLED: ${ENV_TRACING_ENABLED:-true}
                ENV_TRACING_OTLP_ENDPOINT: ${ENV_TRACING_OTLP_ENDPOINT:-http://oullin_tempo:4318}
            build:
                context: .
                dockerfile: ./infra/docker/dockerfile-api
                args:
                    - APP_VERSION=0.0.0.1
                    - APP_HOST_PORT=${ENV_HTTP_PORT}
                    - APP_USER=${ENV_DOCKER_USER}
                    - APP_GROUP=${ENV_DOCKER_USER_GROUP}
                    - APP_DIR=/app
                    - BINARY_NAME=oullin_api
            restart: unless-stopped
            secrets:
                - pg_username
                - pg_password
                - pg_dbname
            depends_on:
                api-db:
                    condition: service_healthy
            expose:
                - ${ENV_HTTP_PORT}
            networks:
                oullin_net: {}
                caddy_net:
                    aliases:
                      - api

        api-db-migrate:
            image: migrate/migrate:v4.19.0
            container_name: oullin_db_migrate
            networks:
                - oullin_net
            volumes:
                - ./database/infra/migrations:/migrations
                - ./database/infra/scripts/run-migration.sh:/run-migration.sh
            secrets:
                - pg_username
                - pg_password
                - pg_dbname
            entrypoint: /run-migration.sh
            command: ["up"]
            depends_on:
                api-db:
                    condition: service_healthy
            restart: no

        api-db:
            # --- Hostinger's Ubuntu VPS hyper-restrictive environments do not allow PostgreSQL containers to behave
            #     predictably. So this option here is a workaround to bypass the apparmor hell configuration that
            #     did not work in spite of days of tries.
            security_opt:
              - apparmor:unconfined

            # --- The container needs to start as root for just a moment to run the chown command and fix the
            #     permissions on the data folder. Immediately after fixing the permissions, our custom
            #     entrypoint script switches to the normal, non-root postgres user to run the actual database.
            user: root

            # Ensure the database always restarts on server reboot or crash.
            restart: always

            image: postgres:17.6-alpine3.22
            container_name: oullin_db

            env_file:
                - .env
            networks:
                - oullin_net

            # --- Use Docker Secrets instead of .env files for credentials.
            #     Docker automatically reads from files specified by these _FILE variables.
            environment:
                POSTGRES_USER_FILE: /run/secrets/pg_username
                POSTGRES_PASSWORD_FILE: /run/secrets/pg_password
                POSTGRES_DB_FILE: /run/secrets/pg_dbname
                PGDATA: /var/lib/postgresql/data/pgdata

            # --- Securing port binding.
            #     Binds the port ONLY to my VPS's localhost (127.0.0.1).
            #
            #     This prevents any direct access to the public internet. My applications running on the same VPS
            #     can connect to it. The ':-5432' provides a fallback default port if ENV_DB_PORT is not set.
            ports:
                - "127.0.0.1:${ENV_DB_PORT:-5432}:5432"

            secrets:
                - pg_username
                - pg_password
                - pg_dbname
            volumes:
                # --- Docker Named Volume for data persistence.
                #     This decouples critical data from the host's file structure, making it more robust & portable.
                - oullin_db_data:/var/lib/postgresql/data

                - ./database/infra/ssl/server.crt:/etc/ssl/certs/server.crt
                - ./database/infra/ssl/server.key:/etc/ssl/private/server.key

                # --- PostgreSQL setup scripts and configuration files as read-only (:ro) for security.
                - ./database/infra/config/postgresql.conf:/etc/postgresql/postgresql.conf:ro
                - ./database/infra/scripts/healthcheck.sh:/healthcheck.sh:ro
                - ./database/infra/scripts/postgres-entrypoint.sh:/postgres-entrypoint.sh

            entrypoint: ["/postgres-entrypoint.sh"]

            logging:
                driver: "json-file"
                options:
                    max-file: 20
                    max-size: 10M

            healthcheck:
                test: ["CMD", "/healthcheck.sh"]
                interval: 10s
                timeout: 5s
                retries: 5
